{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# parses the HTML formatted reviews \n",
    "from bs4 import BeautifulSoup\n",
    "# regular expressions\n",
    "import re\n",
    "# natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "#nltk.download()  \n",
    "\n",
    "# natural language toolkit\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# save and load models and data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews and 50000 unlabeled reviews.\n"
     ]
    }
   ],
   "source": [
    "# Read data from files \n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "if OUTPUT:\n",
    "    print(\"Read {} labeled train reviews, {} labeled test reviews \" \\\n",
    "        \"and {} unlabeled reviews.\".format(\n",
    "        train[\"review\"].size,\n",
    "        test[\"review\"].size, \n",
    "        unlabeled_train[\"review\"].size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review, features=\"html.parser\").get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use stemming instead of lemmatization\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# porter_stemmer = PorterStemmer()\n",
    "# porter_stemmer.stem(w)\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    \"\"\"\n",
    "    Converts a raw IMDb review into an string of (meaningful) words.\n",
    "    :param raw_review: String (raw movie review)\n",
    "    :return: String (preprocessed movie review)\n",
    "    \"\"\"\n",
    "    # remove HTML tags\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    # remove non-letters (esp. digits and punctuation) vie reg. expr.  \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    # convert to lower case and split into words\n",
    "    words = letters_only.lower().split()                             \n",
    "    # sets are faster searched than lists\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # remove stop words and use a lemmatizer to get word stems\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    meaningful_words = [wordnet_lemmatizer.lemmatize(w) for w in words \\\n",
    "                        if w not in stops]\n",
    "    # join the set of words into a space-separated string\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sentences = pickle.load(open(\"./models/sentences.pickle\", \"rb\"))\n",
    "except (OSError, IOError) as e:\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "    \n",
    "    if OUTPUT:\n",
    "        print(\"Parsing sentences from training set\")\n",
    "    for review in train[\"review\"]:\n",
    "        sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "    if OUTPUT:\n",
    "        print(\"Parsing sentences from unlabeled set\")\n",
    "    for review in unlabeled_train[\"review\"]:\n",
    "        sentences += review_to_sentences(review, tokenizer)\n",
    "    pickle.dump(sentences, open(\"./models/sentences.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments = pd.read_csv(\"data/test_sentiments.csv\", header=0, delimiter=\",\", quoting=3)\n",
    "test[\"id\"] = test[\"id\"].apply(lambda x: x.replace('\"', ''))\n",
    "test = test.join(test_sentiments.set_index(\"id\"), on=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
