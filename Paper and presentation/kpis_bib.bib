@inproceedings{Noguishe,
	title={Deep convolutional neural networks for sentiment analysis of short texts},
	author={dos Santos, Cicero and Gatti, Maira},
	booktitle={Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
	pages={69--78},
	year={2014}
}

@article{Pang,
	title={Opinion mining and sentiment analysis},
	author={Pang, Bo and Lee, Lillian and others},
	journal={Foundations and Trends{\textregistered} in Information Retrieval},
	year={2008},
}

@inproceedings{Yang,
	title={A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts},
	author={Pang, Bo and Lee, Lillian},
	booktitle={Proceedings of the 42nd annual meeting on Association for Computational Linguistics},
	pages={271},
	year={2004},
}

@inproceedings{Pak,
	title={Twitter as a corpus for sentiment analysis and opinion mining.},
	author={Pak, Alexander and Paroubek, Patrick},
	year={2010}
}

@article{Kaggle,
	title = {Bag of Words Meets Bags of Popcorn},
	year = {2015},
	journal = {Kaggle Inc.},
	url = {https://www.kaggle.com/c/word2vec-nlp-tutorial#description},
}

@misc{keras,
	title = {Keras: The Python Deep Learning Library},
	howpublished = {\url{https://keras.io/}}
}

@misc{python,
	title = {Python Software Foundation},
	howpublished = {\url{https://www.python.org/}}
}

@misc{imdb_data,
	title = {IMBd data files},
	howpublished = {\url{https://datasets.imdbws.com/}}
}

@article{word2vec,
  author    = {Tomas Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  journal   = {CoRR},
  volume    = {abs/1301.3781},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  archivePrefix = {arXiv},
  eprint    = {1301.3781},
  timestamp = {Mon, 13 Aug 2018 16:48:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1301-3781},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cnn,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{cnn2,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@article{lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

@article{mlp,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@misc{embedding_stackexchange,
	title = {Stackexchange: How the embedding layer is trained in Keras Embedding layer},
	howpublished = {\url{https://stats.stackexchange.com/questions/324992/how-the-embedding-layer-is-trained-in-keras-embedding-layer}}
}

@Article{embedding_example,
author="L{\'o}pez-S{\'a}nchez, Daniel
and Herrero, Jorge Revuelta
and Arrieta, Ang{\'e}lica Gonz{\'a}lez
and Corchado, Juan M.",
title="Hybridizing metric learning and case-based reasoning for adaptable clickbait detection",
journal="Applied Intelligence",
year="2018",
month="Sep",
day="01",
volume="48",
number="9",
pages="2967--2982",
abstract="The term clickbait is usually used to name web contents which are specifically designed to maximize advertisement monetization, often at the expense of quality and 
exactitude. The rapid proliferation of this type of content has motivated researchers to develop automatic detection methods, to effectively block clickbaits in different application 
domains. In this paper, we introduce a novel clickbait detection method. Our approach leverages state-of-the-art techniques from the fields of deep learning and metric learning, 
integrating them into the Case-Based Reasoning methodology. This provides the model with the ability to learn-over-time, adapting to different users' criteria. Our experimental 
results also evidence that the proposed approach outperforms previous clickbait detection methods by a large margin.",
issn="1573-7497",
doi="10.1007/s10489-017-1109-7",
url="https://doi.org/10.1007/s10489-017-1109-7"
}

@article{neural_networks_overview,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@misc{kaggle_baselines,
	title = {Kaggle: IMDb competition baselines},
	howpublished = {\url{https://www.kaggle.com/c/word2vec-nlp-tutorial#description}}
}

@misc{tensorflow,
	title = {Tensorflow: An open source machine learning framework for everyone},
	howpublished = {\url{https://www.tensorflow.org}}
}
